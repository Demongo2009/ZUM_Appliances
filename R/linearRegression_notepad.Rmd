
TWORZENIE MODELU REGRESJI LINIOWEJ
https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r

Wczytanie danych
```{r}
#sprawdzic czy w zbiorze testowym i trenigowym mean =0 sd=1

train<-read.csv("../data/energy_data_train.csv")

train <- subset(train, select = -c(lights) )
train_attributes <- subset(train, select = -c(logAppliances) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances) )
test_y <- subset(test, select = c( logAppliances) )
logAppliances<-train$logAppliances
train <- cbind(train_attributes, logAppliances)
```

Regrasja liniowa

```{r}
model1 <- lm(logAppliances~.,train,na.action=na.exclude)
summary(model1)
```
predykcja

```{r}
prediction<-predict(model1, newdata = test_attributes)

#residual plot
res = resid(model1)
plot(train$logAppliances, res, 
    ylab="Residuals", xlab="Appliances Energy Usage")
#Step 1 - create the evaluation metrics function


eval_metrics = function(model, y, predictions){
    resids = y - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = as.character(round(summary(model)$r.squared, 5))
    adj_r2 = as.character(round(summary(model)$adj.r.squared, 5))
    print(adj_r2) #Adjusted R-squared
    print(as.character(round(sqrt(sum(resids2)/N), 5))) #RMSE
    return(round(sqrt(sum(resids2)/N), 5)) #RMSE

}
```

```{r}
# Step 2 - predicting and evaluating the model on train data
predictions = predict(model1, newdata = train_attributes)
eval_metrics(model1, train$logAppliances, predictions)


# Step 3 - predicting and evaluating the model on test data
predictions = predict(model1, newdata = test_attributes)
eval_metrics(model1, test_y, predictions)
```
F-test
https://statisticsbyjim.com/regression/interpret-f-test-overall-significance-regression/
https://bookdown.org/ndphillips/YaRrr/comparing-regression-models-with-anova.html

```{r}
library(corrplot)
res <- cor( train, use = "complete.obs")
res[is.na(res)] = 0
res = as.data.frame(res)
res<-abs(subset(res, select = c(logAppliances)))
res_names<-rownames(res)[order(-res)][-1]
res<-res[order(-res),][-1]
rsme=list()
f_stat=list()
p_val=list()
first<-0
for(i in 1:length(res_names)){
    sub_train_attr <-as.data.frame(train[,(res_names)[1:i]])
    sub_train_attr <- cbind(sub_train_attr, logAppliances)
    model1 <- lm(logAppliances~.,sub_train_attr,na.action=na.exclude)
    predictions = predict(model1, newdata = as.data.frame(train[,(res_names)[1:i]]))
    rsme=append(rsme,eval_metrics(model1, train$logAppliances, predictions))
    if(first!=0){
        f_stat<-append(f_stat,anova(prev_model,model1)$F[2])
        #print(anova(prev_model,model1))
        p_val<-append(p_val,anova(prev_model,model1)$Pr[2])
    }
    prev_model<-model1
    first<-1
}
plot(seq( 1:length(res_names)),rsme, xlab="models")
print(f_stat)
plot(seq( 1:(length(res_names)-1)),f_stat, xlab="models")
plot(seq( 1:(length(res_names)-1)),p_val, xlab="models")
```

Likelihood Ratio Test (LRT)
https://finnstats.com/index.php/2021/11/24/likelihood-ratio-test-in-r/

```{r}
library(lmtest)
lrt=list()
p_val=list()
first<-0
for(i in 1:length(res_names)){
    sub_train_attr <-as.data.frame(train[,(res_names)[1:i]])
    sub_train_attr <- cbind(sub_train_attr, logAppliances)
    model1 <- lm(logAppliances~.,sub_train_attr,na.action=na.exclude)
    if(first!=0){
        print(lrtest(model1, prev_model))
        p_val<-append(p_val,lrtest(model1,prev_model)$Pr[2])
        lrt<-append(lrt,lrtest(model1,prev_model)$Chisq[2])
    }
    prev_model<-model1
    first<-1
}
print(f_stat)
plot(seq( 1:(length(res_names)-1)),lrt, xlab="models")
plot(seq( 1:(length(res_names)-1)),p_val, xlab="models")
```

Akaike Information Criterion (AIC)
https://www.scribbr.com/statistics/akaike-information-criterion/

```{r}
library(Rcpp)
library(AICcmodavg)
models=list()
for(i in 1:length(res_names)){
    sub_train_attr <-as.data.frame(train[,(res_names)[1:i]])
    sub_train_attr <- cbind(sub_train_attr, logAppliances)
    models[[i]] <- lm(logAppliances~.,sub_train_attr,na.action=na.exclude)
}
aictab(cand.set = models, modnames =seq( 1:length(res_names)))
```

Bayes Information Criterion (BIC)
https://www.statology.org/bic-in-r/

```{r}
library(flexmix)
bics=list()
for(i in 1:length(res_names)){
    sub_train_attr <-as.data.frame(train[,(res_names)[1:i]])
    sub_train_attr <- cbind(sub_train_attr, logAppliances)
    model1 <- lm(logAppliances~.,sub_train_attr,na.action=na.exclude)
    bics<-append(bics,BIC(model1))
}
plot(seq( 1:(length(res_names))),bics, xlab="models")
```
model withou T_out
```{r}
sub_train_attr <-as.data.frame(cbind(train[,(res_names)[1:4]],train[,(res_names)[6:length(res_names)]]))
sub_train_attr <- cbind(sub_train_attr, logAppliances)
model1 <- lm(logAppliances~.,sub_train_attr,na.action=na.exclude)
predictions = predict(model1, newdata = as.data.frame(cbind(train[,(res_names)[1:4]],train[,(res_names)[6:length(res_names)]])))
rsme=eval_metrics(model1, train$logAppliances, predictions)

prediction<-predict(model1, newdata = test_attributes)

#residual plot
res = resid(model1)
plot(train$logAppliances, res, 
    ylab="Residuals", xlab="Appliances Energy Usage")

# Step 2 - predicting and evaluating the model on train data
predictions = predict(model1, newdata = train_attributes)
eval_metrics(model1, train$logAppliances, predictions)


# Step 3 - predicting and evaluating the model on test data
predictions = predict(model1, newdata = test_attributes)
eval_metrics(model1, test_y, predictions)
```

